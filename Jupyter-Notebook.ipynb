{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd520b4",
   "metadata": {},
   "source": [
    "# Gemini RAG Chatbot Advanced Model\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) approach to query PDFs using Google's Gemini API. It extracts text (both structured and raw), splits the text into meaningful chunks, creates vector embeddings using a Sentence Transformer, builds a FAISS index for fast search, applies BM25 filtering, and finally generates answers using Gemini's API.\n",
    "\n",
    "## Features\n",
    "- Structured PDF text extraction with fallback to raw extraction\n",
    "- Technical section tagging and organization\n",
    "- Adaptive text chunking based on semantic coherence\n",
    "- Vector search using FAISS HNSW index\n",
    "- BM25 filtering for improved relevance\n",
    "- Answer generation using Gemini 2.0 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac41af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'gemini_rag_requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 100.9 kB/s eta 0:02:07\n",
      "     --------------------------------------- 0.0/12.8 MB 131.3 kB/s eta 0:01:38\n",
      "     --------------------------------------- 0.1/12.8 MB 262.6 kB/s eta 0:00:49\n",
      "      -------------------------------------- 0.3/12.8 MB 884.2 kB/s eta 0:00:15\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.4/12.8 MB 4.7 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.0/12.8 MB 5.6 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 7.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.2/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.2/12.8 MB 8.9 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.8/12.8 MB 9.1 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.1/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.6/12.8 MB 14.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.2/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.0/12.8 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.4/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 13.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 13.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -r gemini_requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cedccf",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup\n",
    "This cell imports all necessary libraries, configures logging, and loads environment variables (like the Gemini API key). It also initializes the spaCy NLP model and the Sentence Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3df71df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "import pymupdf  # For raw PDF text extraction\n",
    "import spacy  # For NLP tasks\n",
    "import numpy as np\n",
    "import faiss  # For fast vector search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"gemini_benchmark.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# Initialize models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Global storage for processed PDF data\n",
    "pdf_data = {\n",
    "    \"filename\": None,\n",
    "    \"raw_text\": None,\n",
    "    \"tagged_sections\": None,\n",
    "    \"chunks\": None,\n",
    "    \"index\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e77430",
   "metadata": {},
   "source": [
    "## PDF Text Extraction Functions\n",
    "These functions handle different aspects of PDF text extraction:\n",
    "- `extract_text_from_pdf`: Basic text extraction using PyMuPDF\n",
    "- `extract_structured_content`: Structured extraction using Unstructured\n",
    "- `tag_sections_technical`: Section tagging for technical documents\n",
    "- `robust_extract_text`: Combined approach with fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d9031f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path: str) -> str:\n",
    "    \"\"\"Basic text extraction using PyMuPDF.\"\"\"\n",
    "    doc = pymupdf.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_structured_content(file_path: str):\n",
    "    \"\"\"Structured extraction using Unstructured.\"\"\"\n",
    "    elements = partition_pdf(filename=file_path)\n",
    "    structured_data = []\n",
    "    for element in elements:\n",
    "        structured_data.append({\n",
    "            \"type\": element.type,\n",
    "            \"text\": element.text.strip(),\n",
    "        })\n",
    "    return structured_data\n",
    "\n",
    "def tag_sections_technical(structured_elements):\n",
    "    \"\"\"Tags technical sections in the document.\"\"\"\n",
    "    section_pattern = re.compile(\n",
    "        r\"(Abstract|Introduction|Related Work|Background|Methodology|Approach|Experiments|Results|Discussion|Conclusion|Encoding|CLIP|Text Encoder|Embedding)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    tagged_sections = {}\n",
    "    current_section = None\n",
    "    \n",
    "    for element in structured_elements:\n",
    "        element_type = element.get(\"type\", \"\").lower()\n",
    "        text = element.get(\"text\", \"\")\n",
    "        if element_type in [\"heading\", \"title\"] or section_pattern.search(text):\n",
    "            match = section_pattern.search(text)\n",
    "            new_section = match.group(0).strip() if match else text.strip()\n",
    "            current_section = new_section\n",
    "            if current_section not in tagged_sections:\n",
    "                tagged_sections[current_section] = []\n",
    "        elif current_section:\n",
    "            tagged_sections[current_section].append(text)\n",
    "        else:\n",
    "            tagged_sections.setdefault(\"Body\", []).append(text)\n",
    "    \n",
    "    for section in tagged_sections:\n",
    "        tagged_sections[section] = \"\\n\".join(tagged_sections[section]).strip()\n",
    "    return tagged_sections\n",
    "\n",
    "def robust_extract_text(file_path: str):\n",
    "    \"\"\"Combined extraction with fallback.\"\"\"\n",
    "    try:\n",
    "        structured_elements = extract_structured_content(file_path)\n",
    "        tagged_sections = tag_sections_technical(structured_elements)\n",
    "        combined_text = \"\\n\\n\".join([f\"{section}: {content}\" for section, content in tagged_sections.items()])\n",
    "        if combined_text.strip():\n",
    "            return combined_text, tagged_sections\n",
    "        else:\n",
    "            raise Exception(\"No structured content extracted.\")\n",
    "    except Exception as e:\n",
    "        logging.info(\"Structured extraction failed; using fallback extraction. Error: \" + str(e))\n",
    "        fallback_text = extract_text_from_pdf(file_path)\n",
    "        return fallback_text, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9465ac",
   "metadata": {},
   "source": [
    "## Text Processing Functions\n",
    "These functions handle text chunking, embedding generation, and vector search:\n",
    "- `adaptive_chunk_text_dynamic`: Semantic chunking with dynamic thresholds\n",
    "- `get_embeddings`: Text embedding generation\n",
    "- `build_hnsw_index`: FAISS index construction\n",
    "- `search_index`: Vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fff2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_chunk_text_dynamic(text: str, min_threshold: int = None, factor: float = 1.5, transition_words=None):\n",
    "    \"\"\"Splits text into semantically coherent chunks.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    token_counts = [len(sent.split()) for sent in sentences]\n",
    "    if not token_counts:\n",
    "        return [text]\n",
    "    avg_tokens = sum(token_counts) / len(token_counts)\n",
    "    \n",
    "    if min_threshold is None:\n",
    "        min_threshold = int(avg_tokens)\n",
    "    threshold = int(max(min_threshold, avg_tokens * factor))\n",
    "    \n",
    "    if transition_words is None:\n",
    "        transition_words = [\"however\", \"moreover\", \"furthermore\", \"in conclusion\", \"finally\", \"additionally\"]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_token_count = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sent_tokens = len(sent.split())\n",
    "        if current_chunk:\n",
    "            starts_with_transition = any(sent.lower().startswith(word) for word in transition_words)\n",
    "        else:\n",
    "            starts_with_transition = False\n",
    "        \n",
    "        if (current_token_count + sent_tokens > threshold) or (starts_with_transition and current_token_count > int(threshold * 0.7)):\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent\n",
    "            current_token_count = sent_tokens\n",
    "        else:\n",
    "            current_chunk += \" \" + sent\n",
    "            current_token_count += sent_tokens\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_embeddings(chunks: list) -> np.ndarray:\n",
    "    \"\"\"Generates embeddings for text chunks.\"\"\"\n",
    "    embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
    "    return embeddings.astype(\"float32\")\n",
    "\n",
    "def build_hnsw_index(embeddings: np.ndarray, M: int = 32, efConstruction: int = 40):\n",
    "    \"\"\"Builds a FAISS HNSW index.\"\"\"\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexHNSWFlat(d, M)\n",
    "    index.hnsw.efConstruction = efConstruction\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def search_index(query: str, index, chunks: list, k: int = 5) -> list:\n",
    "    \"\"\"Performs vector similarity search.\"\"\"\n",
    "    start_time = time.time()\n",
    "    query_embedding = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
    "    search_duration = time.time() - start_time\n",
    "    logging.info(f\"HNSW Search Time: {search_duration:.4f} seconds\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c718470",
   "metadata": {},
   "source": [
    "## Answer Generation Functions\n",
    "These functions handle answer generation using Gemini and BM25 filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fd969ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_filter(query, candidate_chunks, threshold=1.0, top_n=2):\n",
    "    \"\"\"\n",
    "    Use BM25 to select the top N most relevant chunks instead of just one.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        candidate_chunks: List of candidate chunks from vector search\n",
    "        threshold: Minimum score to consider\n",
    "        top_n: Number of top chunks to return\n",
    "        \n",
    "    Returns:\n",
    "        List of most relevant chunks\n",
    "    \"\"\"\n",
    "    if not candidate_chunks:\n",
    "        return []\n",
    "    \n",
    "    tokenized_corpus = [doc.lower().split() for doc in candidate_chunks if doc.strip()]\n",
    "    if not tokenized_corpus:\n",
    "        return []\n",
    "    \n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get indices sorted by score in descending order\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    \n",
    "    # Select top N chunks that meet the threshold\n",
    "    selected_chunks = [\n",
    "        candidate_chunks[idx] for idx in sorted_indices[:top_n] \n",
    "        if scores[idx] >= threshold and idx < len(candidate_chunks)\n",
    "    ]\n",
    "    \n",
    "    return selected_chunks\n",
    "\n",
    "def generate_answer_with_gemini(query: str, context: str) -> str:\n",
    "    \"\"\"Generates a well-reasoned answer using Gemini API with enhanced prompt formatting.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an advanced Retrieval-Augmented Generation (RAG) AI assistant specializing in legal document analysis. \n",
    "    Your task is to accurately answer the given query based on the provided legal context.\n",
    "\n",
    "    **Context:** \n",
    "    {context}\n",
    "\n",
    "    **Query:** \n",
    "    {query}\n",
    "\n",
    "    **Instructions:**\n",
    "    - Provide a **clear and direct answer first** (Yes/No with a brief explanation).\n",
    "    - Then, **explain your reasoning** in structured points.\n",
    "    - If the correct context is not provided,say so. \n",
    "    - Ensure proper formatting with **clear paragraph separation**.\n",
    "\n",
    "    **Final Answer:** \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "        response = model.generate_content(prompt)\n",
    "        answer = response.text.strip()\n",
    "    except Exception as e:\n",
    "        answer = f\"Error generating answer: {e}\"\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f1bda",
   "metadata": {},
   "source": [
    "## PDF Processing Function\n",
    "This function combines all the steps to process a PDF document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62bcc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_advanced(pdf_path):\n",
    "    \"\"\"Processes a PDF document through all steps.\"\"\"\n",
    "    extracted_text, tagged_sections = robust_extract_text(pdf_path)\n",
    "    chunks = adaptive_chunk_text_dynamic(extracted_text)\n",
    "    embeddings = get_embeddings(chunks)\n",
    "    faiss_index = build_hnsw_index(embeddings)\n",
    "    return {\n",
    "         \"raw_text\": extracted_text,\n",
    "         \"tagged_sections\": tagged_sections,\n",
    "         \"chunks\": chunks,\n",
    "         \"index\": faiss_index,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ed278",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "Below is an example of how to use the RAG system with a sample PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "602177fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged Sections:\n",
      "Total Chunks: 91\n",
      "Sample Chunk (first 300 characters):\n",
      "STANDARD LEASE AGREEMENT THIS LEASE AGREEMENT hereinafter known as the \"Lease\" is made and entered into this ____ ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving candidate chunks for query: 'Will the landlord be liable for any damages to the tenant or his family?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 52.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNSW Search Time: 0.0300 seconds\n",
      "\n",
      "Candidate Chunks and Similarity Scores:\n",
      "Chunk 1:\n",
      "Similarity Score: 0.613007128238678\n",
      "Text (first 300 characters): If said damage was due to the negligence of the Tenant(s), the Tenant(s) shall be liable to the Landlord for all repairs and for the loss of income due to restoring the property back to a livable condition in addition to any other losses that can be proved by the Landlord....\n",
      "\n",
      "Chunk 2:\n",
      "Similarity Score: 0.7047845125198364\n",
      "Text (first 300 characters): INDEMNIFICATION.â€‹ Landlord shall not be liable for any injury to the tenant, tenantâ€™s family, guests, or employees or to any person entering the Property and shall not be liable for any damage to the building in which the Property is located or to goods or equipment, or to the structure or equipment...\n",
      "\n",
      "Chunk 3:\n",
      "Similarity Score: 0.7810131907463074\n",
      "Text (first 300 characters): DEFAULT.â€‹ If Landlord breaches this Lease, Tenant may seek any relief provided by law....\n",
      "\n",
      "Chunk 4:\n",
      "Similarity Score: 0.8533125519752502\n",
      "Text (first 300 characters): Tenant is responsible and liable for any damage or required cleaning to the Property caused by any authorized or unauthorized animal and for all costs Landlord may incur in removing or causing any animal to be removed....\n",
      "\n",
      "BM25 Filtered Candidate Found.\n",
      "\n",
      "Final Context for LLM Prompt:\n",
      "INDEMNIFICATION.â€‹ Landlord shall not be liable for any injury to the tenant, tenantâ€™s family, guests, or employees or to any person entering the Property and shall not be liable for any damage to the building in which the Property is located or to goods or equipment, or to the structure or equipment of the structure in which the Property is located, and Tenant hereby agrees to indemnify, defend, and hold Landlord harmless from any and all claims or assertions of every kind and nature. \n",
      "\n",
      "Generated Answer:\n",
      "**Final Answer:**\n",
      "\n",
      "No. The lease explicitly states the landlord will not be liable for injury to the tenant, tenant's family, or guests.\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "* **Explicit Liability Waiver:** The provided clause clearly states that the landlord \"shall not be liable for any injury to the tenant, tenantâ€™s family, guests, or employees...\" This directly addresses the query and removes liability from the landlord for injuries to the specified individuals.\n",
      "* **Indemnification Clause:** The tenant agrees to \"indemnify, defend, and hold Landlord harmless\" from any claims. This further reinforces the landlord's protection from liability, as the tenant assumes the responsibility for defending against and covering any potential costs associated with such claims.\n",
      "* **Scope of Coverage:** The clause covers not only injury to people but also damage to the building, goods, equipment, and structure. This broad scope emphasizes the landlord's intention to avoid liability for a wide range of potential issues.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to a sample PDF (adjust as needed)\n",
    "test_pdf_path = r\"C:\\Users\\amaan\\Downloads\\Lease Agreement PDF.pdf\"\n",
    "\n",
    "# Step 1: Extract and tag text from the PDF.\n",
    "extracted_text, tagged_sections = robust_extract_text(test_pdf_path)\n",
    "print(\"Tagged Sections:\")\n",
    "for section, content in tagged_sections.items():\n",
    "    print(f\"--- {section} ---\\n{content[:300]}...\\n\")  # Print first 300 characters for brevity\n",
    "\n",
    "# Step 2: Adaptive Chunking.\n",
    "chunks = adaptive_chunk_text_dynamic(extracted_text)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(\"Sample Chunk (first 300 characters):\")\n",
    "print(chunks[0][:300], \"...\\n\")\n",
    "\n",
    "# Step 3: Generate embeddings and build the FAISS index.\n",
    "embeddings = get_embeddings(chunks)\n",
    "index = build_hnsw_index(embeddings)\n",
    "\n",
    "# Step 4: Retrieve candidate chunks with similarity scores.\n",
    "query = \"Will the landlord be liable for any damages to the tenant or his family?\"\n",
    "print(f\"Retrieving candidate chunks for query: '{query}'\")\n",
    "start_time = time.time()\n",
    "query_embedding = embed_model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "distances, indices = index.search(query_embedding, k=4)\n",
    "search_duration = time.time() - start_time\n",
    "print(f\"HNSW Search Time: {search_duration:.4f} seconds\\n\")\n",
    "\n",
    "candidate_chunks = []\n",
    "print(\"Candidate Chunks and Similarity Scores:\")\n",
    "for i, (idx, score) in enumerate(zip(indices[0], distances[0])):\n",
    "    if idx < len(chunks):\n",
    "         candidate_chunks.append(chunks[idx])\n",
    "         print(f\"Chunk {i+1}:\")\n",
    "         print(f\"Similarity Score: {score}\")\n",
    "         print(f\"Text (first 300 characters): {chunks[idx][:300]}...\\n\")\n",
    "\n",
    "# Step 5: Optionally filter candidate chunks using BM25.\n",
    "filtered_chunks = bm25_filter(query, candidate_chunks, threshold=1.0)\n",
    "if filtered_chunks:\n",
    "    final_context = filtered_chunks[0]  # Use the top BM25 candidate.\n",
    "    print(\"BM25 Filtered Candidate Found.\\n\")\n",
    "else:\n",
    "    final_context = \"\\n\\n\".join(candidate_chunks)\n",
    "    print(\"No BM25 candidates passed the threshold. Using all candidate chunks.\\n\")\n",
    "\n",
    "print(\"Final Context for LLM Prompt:\")\n",
    "print(final_context, \"\\n\")\n",
    "\n",
    "# Step 6: Generate an answer using the final context.\n",
    "answer = generate_answer_with_gemini(query, final_context)\n",
    "print(\"Generated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be8f081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d0f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
